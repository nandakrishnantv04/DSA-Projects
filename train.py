# -*- coding: utf-8 -*-
"""Model Training Script

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oGQodggLIrQMMG_TxfPaCFsD1iAMwAyM
"""

# train.py

import pandas as pd
import numpy as np
import joblib
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from utils import create_preprocessor, get_models, evaluate_model

# --- CONFIGURATION ---
DATA_PATH = 'WA_Fn-UseC_-Telco-Customer-Churn.csv'
ARTIFACTS_DIR = Path('artifacts')
ARTIFACTS_DIR.mkdir(exist_ok=True)
RANDOM_STATE = 42
# ---------------------

def load_and_clean_data(file_path):
    """Loads, cleans, and prepares the dataset."""
    print("Loading and cleaning data...")
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError:
        print(f"FATAL: Data file not found at: {file_path}. Please place it in the same directory.")
        return pd.DataFrame()

    # Cleaning TotalCharges
    df['TotalCharges'] = df['TotalCharges'].replace(' ', np.nan)
    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'])
    df.dropna(subset=['TotalCharges'], inplace=True)

    # Encode target
    df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})

    return df

def train_and_save():
    """Executes the training workflow and saves the artifacts."""
    df = load_and_clean_data(DATA_PATH)
    if df.empty:
        return

    # Prepare features and target
    X = df.drop(['customerID', 'Churn'], axis=1)
    y = df['Churn']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y
    )

    preprocessor = create_preprocessor()
    models = get_models()

    best_auc = -1
    best_model_name = ""
    best_pipeline = None
    all_results = {}

    print(f"Starting training of {len(models)} models...")

    # Iterate through models
    for name, model in models.items():
        print(f"\nTraining {name}...")

        # Create the full pipeline (Preprocessor + Model)
        model_pipeline = Pipeline(steps=[
            ('preprocessor', preprocessor),
            ('classifier', model)
        ])

        # Fit the pipeline
        model_pipeline.fit(X_train, y_train)

        # Evaluate
        y_proba = model_pipeline.predict_proba(X_test)[:, 1]
        y_pred = model_pipeline.predict(X_test)

        # Get metrics
        results = evaluate_model(y_test, y_proba, y_pred, name)
        all_results[name] = results

        auc_score = results['AUC Score']
        print(f"-> {name} AUC Score: {auc_score:.4f}")

        # Track the best model
        if auc_score > best_auc:
            best_auc = auc_score
            best_model_name = name
            best_pipeline = model_pipeline

    print(f"\n--- Training Complete ---")
    print(f"Best Model Selected: {best_model_name} with AUC: {best_auc:.4f}")

    # --- Save Artifacts ---
    try:
        # 1. Save the entire best pipeline
        joblib.dump(best_pipeline, ARTIFACTS_DIR / 'best_pipeline.joblib')

        # 2. Save the feature names for interpretation
        preprocessor_fitted = best_pipeline['preprocessor'].fit(X_train)
        feature_names = preprocessor_fitted.get_feature_names_out()
        joblib.dump(feature_names, ARTIFACTS_DIR / 'feature_names.joblib')

        # 3. Save all evaluation results for display in the app
        joblib.dump(all_results, ARTIFACTS_DIR / 'all_results.joblib')

        print(f"Artifacts successfully saved in the '{ARTIFACTS_DIR}' folder.")
    except Exception as e:
        print(f"Error saving artifacts: {e}")

if __name__ == '__main__':
    train_and_save()